we have many different kind of problems in data and we need to do something about it

also, the type of data may not be adequate

how can i decide when to just drop the row/column instead of inferring the datapoint?

...

# Data type conversions

...

### Ordinal to numeric

with this conversion we're adding an unwanted semantic. We're defining how much better ok is better than awful etc.

This can change the behaviour of a classifier. For example:

- knn is affected
- decision trees aren't affected

### Discretization

The choice of the (optimal) thresholds can emerge from data

- equal width: mi basta decidere il numero di buckets
  - this choice doesn't respect the data!
  - we're putting in the same bucket datapoints conceptually belonging to different sets
- equal frequency:
  - se ho 100 datapoints, e voglio 4 buckets, metto i primi 25 nel primo bucket, e cos√¨ via
  - un po' meglio di prima ma di nuovo non rispetta granch√® i dati se la distribuzione √® sbilanciata
- k-means: fuoco

# Sampling

Se il mio dataset √® enorme, lavorare con il dataset nella sua interezza potrebbe essere troppo costoso

Una buona idea √® lavorare inizialemente con un sottoinsieme del dataset ottenuto tramite sampling, ad esempio per la stima degli iperparametri.

Una volta soddisfatti con i risultati sul sample si pu√≤ passare all'intero dataset

In general, we will consider sampling without replacement (vedi splitter per crossvalidation)

### Sample size relation with missing class

nel caso di training-test split con crossvalidation √® importante considerare la dimensione dei singoli bucket ottenuti da crossvalidation nel training set

- devono essere abbastanza grandi da contenere con buona probabilit√† almeno una istanza per ogni classe
- **questo ci aiuta a definire il numero di bucket nella crossvalidation**

# Feature creation

...

**NB**: New features: e.g. volume and weight to density

- questo pu√≤ essere importante dato che la densit√† non √® ottenuta da una relazione lineare tra peso e volume
- molti classificatori per√≤ combinano linearmente peso e volume per fare le loro scelte
- se la densit√† fosse una feature importante del mio problema, senza fare feature creation il modello non riuscirebbe a classificare utilizzando questa metrica
  - non riesce a fare decisioni basate su densit√† combinando linearmente peso e volume

moving average reduces the random effect in data (in particular for time related information)

...

# Data transformation

the scale of the features can influence machine learning techniques

- algorythms that are influenced by distance
- algorythms that use gradient descent

- min-max scaling
- stadardization
  - less influenced by outliars
- funzioni qualsiasi
  - tipicamente cambiano la distribuzione dei dati

  Normalization √® un termine overloaded
  - spesso fa riferimento al MinMaxScaler
  - in scikit-learn normalizza each data-row to unit norm

Nota: min-max scaling should always be done when using neural networks (non ha spiegato perch√® ma mi fido)

# Imbalanced dataset

tipically underepresented classes have little influence on the performance of the model

this is a problem if i'm interested on the performance on minority classes

...

we can weigh the classes in our classifiers (?)

- vedi estimators in scikit-learn

oversampling

- we consider examples of underepresented classes many times
- we do not introduce new information

data augmentation

- we synthetize new plausible data for underepresente classes

**NB**: with oversampling, undersampling and data augmentation it's important to not touch the test set because we want to measure performance on real data

undersampling selects horizontal portions of the data

feature selection select vertical portions of the data

# Feature selection

...

## The curse of dimensionality

al crescere del numero di features (dimensionalit√†), algoritmi che discriminano in base a distanze, diventano sempre meno efficaci

- In alta dimensionalit√†, la distanza massima e quella minima tra punti tendono ad avvicinarsi.
- Le distanze perdono significato

### Filter and Wrapper methods

Nella Feature Subset Selection esistono diversi modi per scegliere un sottoinsieme di attributi utili a migliorare prestazioni, e generalizzazione di un modello.

```
dependant variable == variabile che vogliamo predire (che ci si aspetta dipenda in qualche modo dalle feature)
```

Filter e Wrapper ‚Äî rappresentano due grandi famiglie di tecniche per questo scopo

**Filter Approaches**

I filter selezionano le feature prima dell‚Äôapplicazione di qualsiasi algoritmo di data mining o machine learning.

Esempio: Correlation-based selection -> si scelgono le feature pi√π correlate con la target.

**Wrapper approaches**

I wrapper selezionano le feature utilizzando direttamente il modello come ‚Äúvalutatore‚Äù. Per ogni sottoinsieme testato, il modello viene addestrato e valutato: il sottoinsieme che produce le performance migliori viene scelto

### Difference between Filter and Wrapper methods

- Filter methods measure the relevance of features by their correlation with dependent variable
- while wrapper methods measure the usefulness of a subset of feature by actually training a model on it

...

in the last line there's obvious relationship between the features but the correlation is 0

- this is because of simmetry and non-linear relationships

## dimensionality-reduction

Instead of considering which subset of attributes is to be ignored it is possible to map the dataset into a new space with fewer attributes

### PCA Principal Component Analysis

```chatgpt
In PCA, **‚Äúcapturing the variability of the data‚Äù** significa:

‚û°Ô∏è **Rappresentare quanta parte dell‚Äôinformazione contenuta nei dati originali riusciamo a mantenere proiettandoli su una nuova direzione (una nuova dimensione).**

### Pi√π concretamente:

* La **variabilit√†** √® misurata tramite la **varianza**.
* Una direzione (o principal component) **cattura molta variabilit√†** se, quando proiettiamo i dati su quella direzione, i valori proiettati mostrano **alta varianza**.
* Alta varianza = punti pi√π ‚Äúsparpagliati‚Äù lungo quella direzione ‚Üí quella direzione contiene molta informazione sulla struttura dei dati.
* Proiettare i dati su una direzione significa considerarne solo la componente relativa a quella direzione


Le nuove dimensioni create dalla PCA:

1. Sono ordinate dalla maggiore alla minore varianza catturata.
2. Le prime poche componenti spesso spiegano la maggior parte dell‚Äôinformazione utile nei dati originali.
3. Questo permette di ridurre la dimensionalit√† senza perdere troppo contenuto informativo.

### Un esempio intuitivo

Immagina un insieme di punti disposti come un‚Äôellisse molto allungata:

* La direzione dell‚Äôasse lungo dell‚Äôellisse ha molta variabilit√† ‚Üí **prima componente principale**.
* La direzione corta ne ha meno ‚Üí **seconda componente**.

Se proietti i dati sulla prima direzione, preservi gi√† gran parte della forma dell‚Äôellisse ‚Üí hai **catturato molta variabilit√†**.

In breve: **"Catturare la variabilit√†" = mantenere quanto pi√π possibile la varianza dei dati originali quando li rappresenti in meno dimensioni.**

INOLTRE

In PCA non utilizzi le direzioni gi√† presenti nel dataset (cio√® gli assi originali degli attributi).
üëâ Devi calcolare delle nuove direzioni, chiamate componenti principali.

Gli assi originali non sono scelti per massimizzare la variabilit√†. Sono solo le coordinate in cui i dati sono stati raccolti.

PCA cerca invece le direzioni ‚Äúmigliori‚Äù per descrivere i dati, cio√®: quelle lungo cui i dati variano di pi√π (massima varianza) e che siano tra loro ortogonali (indipendenti)

Per calcolare queste nuove direzioni si fanno delle operazioni sulla matrice di covarianza e analisi sui suoi autovettori/valori
```

PCA √® computationally intensive, esistono forme approssimate pi√π fattibili per dataset grandi

## MultiDimensional Scaling

tecnica di visualizzazione

MDS is a technique used to visualize high-dimensional data in a low-dimensional space

Fits the projection of the elements into a m dimensional space in such a way that the distances among the elements are preserved

# Scikit-learn

score, quanto √® importante la feature per il target

pvalue = misura di quanto lo score √® affidabile

- es: lancio una moneta 10 volte e ottengo 9 teste e 1 croce. p(testa) = 0.9?
- l'affidabilit√† dipende dal numero di volte che lancio la moneta. Il pvalue mi d√† una misura di affidabilit√† che in questo caso dipende dal numero di lanci

## Recursive Feature Elimination Wrapper Method
