# Model selection

like any optimization problem, abbiamo una funzione obiettivo: accuracy del modello

con model selection dobbiamo scegliere sia il modello sia i suoi iperparametri che massimizzano l'accuracy

## Oil slick example

attenzione a non ignorare false negatives

# Meaning of the test error

...

L’errore del test set come stima dell’errore reale

- If the test set error ratio is x, we should expect a run-time error x ± ???
- Hai un valore osservato di errore sul test set, chiamato x.
- Ma vuoi sapere quanto fidarti di questo valore: cioè, quanto può variare rispetto al “vero” errore reale (sul mondo intero).

Il simbolo x ± ??? rappresenta l’intervallo di confidenza, cioè un range di valori dentro cui ci aspettiamo che cada il vero errore, con una certa probabilità (ad esempio 95%).

Ogni predizione può essere vista come un esperimento Bernoulliano:

- Successo = predizione corretta
- Fallimento = errore
- Se hai N elementi nel test set, hai N esperimenti indipendenti con probabilità di successo p (cioè probabilità che il modello indovini).
- f = errore empirico = S / N; con S = numero di errori nel test set
  - è una stima della vera probabilità di errore p

La differenza tra f (errore osservato) e p (errore reale) è dovuta al rumore statistico.

- Se il test set è grande (N ≥ 30), la distribuzione del rumore può essere approssimata con una distribuzione normale (grazie al teorema del limite centrale).
- Abbiamo la conferma guardando la formula: abbiamo un N a denominatore. Questo significa che maggiore è la dimensione del test set, minore è l'incertezza del mio intervallo di confidenza (+- part)

Definiamo quindi un intervallo di confidenza in cui ricade p con un certo livello di confidenza (1-alpha)

- Il **pessimistic error** si ottiene prendendo il valore più alto dell’intervallo (+ invece di ±) e, come dice il nome, rappresenta la stima pessimistica della mia probabilità di errore p
- Il pessimistic error è l'error che è bene utilizzare nella stima della bontà di un modello

slide 105 ci dice che:

- se voglio un higher confidence level (1-alpha piccolo)
- l'errore pessimistico è più grande (l'intervallo di confidenza cresce)
- se però facciamo crescere la dimensione del test set N, la differenza tra confidence level diversa si attenua
- inoltre l'errore diminuisce in termini assoluti

**riassumendo**:

Hai ottenuto un errore empirico f dal test set. Ma vuoi sapere quanto è affidabile questa stima.

Costruisci un intervallo di confidenza intorno a f, che ti dice: “Con probabilità 1 − α (es. 95%) il vero errore reale p cade dentro questo intervallo.”

Esempio numerico

Supponi:

- N = 1000 esempi nel test set
- f = 0.08 (8% di errore)
- confidenza = 95% → z = 1.96

Usando la formula del Wilson interval: Errore reale p ∈ [ 0.065 , 0.097 ]

cioè: “Stimiamo che l’errore reale sia 8%, ma con il 95% di confidenza è tra 6.5% e 9.7%.”

## pruning

**statistical pruning**:
i nodi profondi hanno N piccolo e quindi più incertezza

però possiamo fare anche in maniera più ignorante

# Dataset splits

misure di accuracy vengono utilizzate in due modi:

- ottimizzazione:
  - otteniamo una misura con cui confrontare modelli diversi e configurazioni diverse degli iperparametri
- performance:
  - ottenere una stima di quanto il nostro modello sbaglia

slide 118 importante:

- train/test split
  - semplice e veloce
  - non molto reliable dato che ci sono poche entry nel test set
  - Bisogna trovare un tradeoff tra training set e test set
    - più entry ci sono nel training set, meglio il modello impara
    - più entry ci sono nel test set, migliore è la stima dell'errore
- train/validation/test split
  - Alleni il modello sul training set.
  - Misuri l’errore sul validation set.
  - Cambi iperparametri (es. learning rate, profondità, ecc.) finché le prestazioni migliorano.
  - Quando hai trovato la configurazione migliore, testi una sola volta sul test set per stimare l’errore reale.
- **crossvalidation**
  - Cross-validation ti permette di riutilizzare tutti i dati sia per allenare che per testare (validare), ma in momenti diversi.
    - Così ottieni una stima stabile dell’errore senza sacrificare dati.
  - ogni elemento del dataset viene usato:
    - number of subsets - 1 volte per il training
    - una volta per il testing
  - optimal use of the supervised data
  - **NB**: the error generated by crossvalidation is more reliable (less uncertainty), less prone to randomness (facciamo una media)
    - crossvalidation serve ad ottenere una stima dell'errore più certa
- **NB**: nella stima degli iperparametri potremmo applicare crossvalidation con varie configurazioni degli iperparametri in maniera da trovare la configurazione migliore.
  - tuttavia, se utilizziamo crossvalidation sull'intero dataset, siamo suscettibili ad overfitting degli iperparametri dato che stiamo utilizzando gli stessi dati per training and testing
  - posso fare uno split e fare crossvalidation sul training set e testare la mia nuova configurazione degli iperparametri con il test set

**NB**: The split should be as random as possible. Quando dividi un dataset in training, validation e test, l’obiettivo è che ogni insieme rappresenti bene la distribuzione reale dei dati e che non contenga bias dovuti all’ordine o ad altri fattori strutturali.

Se non usi la casualità (per esempio, prendi i primi 80% dei dati per il training e gli ultimi 20% per il test), rischi che:

- il modello “veda” solo un certo tipo di esempi in training,
- e che il test contenga casi molto diversi → valutazione non realistica.

Attenzione però alla casualità:

- It may happen that the proportion of classes in the supervised dataset X is altered in the Training and Test sets, to prevent such cases the statistical sampling technique of **stratification** ensures the maintenance of the proportion of classes
- stratify=y in scikit-learn

Con crossvalidation, ci si potrebbe chiere quale delle k istanze del modello usare. La risposta è: non esiste una “singola istanza” del modello

- ne addestri una nuova in ogni fold (quindi k istanze diverse se fai k-fold cross-validation).

- Cosa succede in ogni fold
  - Dividi il dataset in k parti (fold).
  - Per ciascun fold i:
    - Addestri una nuova istanza del modello sui k−1 fold di training.
    - Valuti quella istanza sul fold i (il test di turno).
  - Quindi se usi, ad esempio, k=5:
    - Allenerai 5 modelli distinti (ognuno addestrato su un sottoinsieme leggermente diverso dei dati).
    - Ottieni 5 misure di errore (una per ciascun modello).
    - Calcoli la media → stima complessiva dell’errore di generalizzazione.

- Quindi quale modello “uso davvero” alla fine?
  - Caso A: vuoi solo stimare l’errore del modello
    - Scopo: valutare la qualità attesa del modello e non ancora usarlo per fare previsioni reali.
    - Usi tutte le k istanze solo per calcolare l’errore medio. Poi le scarti.
    - Dopo la CV, addestri una nuova istanza finale del modello su tutto il dataset (train+test combinati).
    - ora puoi sfruttare tutti i dati disponibili per la versione definitiva del modello dato che hai già calcolato l'accuracy con la crossvalidation
  - Caso B: stai scegliendo il miglior modello / iperparametri
    - Scopo: trovare il modello o la configurazione migliore.
    - Usi cross-validation per confrontare diversi modelli (o set di iperparametri).
    - Scegli la combinazione con l’errore medio più basso.
    - Poi riaddestri un’unica istanza del modello su tutto il dataset usando quei parametri ottimali.

## Model selection

in breve prova modelli diversi e prendi il migliore

# Kinds of accuracy measures

confusion matrix

... optimizing f1-score means balancing precision and recall

what measure should we use

multiclass case

...

if we want to use scikit learn functions like gridsearch to optimize something other than accuracy in a multiclass model we need a sort of average value of the measure for the classes

- if the classes are balanced it doesn't really matter
- if they are unbalanced it matters

macroaverage da la stessa importanza a tutte le classi; underepresented o meno

weighted average mi permette di dare meno importanza alle classi meno rappresentate

microaverage ci interessa di meno

**NB**: the scoring is decided depending on your need! Ad esempio, è il buisness owner che ti viene a dire che lui vuole il weighted recall migliore possibile. NON possiamo scegliere lo scoring che ci dà il risultato migliore possibile

## Beyond pure counting

is our classifier making correct predictions by chance?

...

definiamo un random classifier che produce la stessa proporzione di classi di C

definiamo k(C)

- un altra misura (insieme ad accuracy, precision, ...) che ci permette di trovare la miglior configurazione degli iperparametri per i nostri scopi

nell'esempio della malattia: se C ha accuracy 98% -> k(c) = 0

k score becomes good after 0.6

MCC non ci interessa

## Cost sensitive learning

if we have more information such as

- the cost of errors
- the cost of good prediction
- the cost of bad prediction

we can define our own error measures (cost function)

Several alternatives ma quella che ci interessa:
alterate the proportion of classes in the supervised data, duplicating the examples for which the classification error is higher

- In this way, the classifier will became more able to classify the classes for which the classification error cost is higher
- This solution is useful also when the classes are imbalanced, that is the frequencies of the class labels in X are not equal

# Evaluation of a Probabilistic classifier

interessante utilizzare la probabilità predetta come strumento di ranking quando si lavora in batch-mode

- ho un batch di oggetti, filtro solo quelli che hanno una probabilità che rispetta una threshold ed ottengo un subset

### Lift chart

the lift tells us how much a predicted probability is better than a random choice

utile quando si lavora in batch mode

### ROC curve

qua non lavoro in batch mode ma caso per caso in una crisp mode

- here i want to transform the probabilistic output into a crisp one

voglio anche configurare quanto è facile/difficile data una probabilità, assegnare l'oggetto ad una classe

Calcolare la ROC curve mi permette di vedere come il variare della threshold mi fa crescere il numero di positives e false positives che catturo (e viceversa)

- molto utile se ad esempio false positives mi fanno molto male e magari false negatives non così tanto
