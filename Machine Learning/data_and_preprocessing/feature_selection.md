# Feature selection

why is it used?

1. Sometimes less is more:
    - It can enable the machine learning algorithm to train faster
        - ad esempio, mitiga curse of dimensionality dato che fa **dimensionality reduction**
        - oppure, semplicemente dato che ci sono meno colonne reduces time and memory complexity of the mining algorithm
    - It can reduce the complexity of a model and makes it easier to interpret (eg. with visualizations)
    - It can improve the accuracy of a model if the right subset is chosen
    - **It can reduce overfitting**
        - eliminando feature non importanti/non predittive quest'ultime non possono venire imparate e non possono portare a predizioni sbagliate

2. Some attributes can be problematic
    - may be redundant if correlation between the two is high
    - can be misleading ... non ci interessa come
    - **√® bene tagliare via alcuni attributi problematici, una sorta di data cleaning**

### The curse of dimensionality

al crescere del numero di features (dimensionalit√†), algoritmi che discriminano in base a distanze, diventano sempre meno efficaci

- In alta dimensionalit√†, lo spazio diventa sparsamente popolato (la densit√† dei punti tende a diminuire)
- questo significa che la distanza massima e quella minima tra punti tendono ad avvicinarsi.
- in altri termini, **le distanze diventa sempre pi√π simili tra di lore -> Le distanze perdono significato**

## Come si fa?

Innanzitutto, possiamo sicuramente:

- eliminare attributi ridondanti
  - sia quelli fortemente correlati tra di loro, che quelli derivabili da altri
- eliminare gli attributi irrilevanti, ovvero quelli che non hanno alcun poter predittivo sul target (vedi codice fiscale e ricchezza)
- eliminare attributi con poco variabilit√†
  - se √® un attributo √® pi√π o meno sempre uguale across l'intero dataset allora non serve a molto

Da qui in avanti comincia la parte pi√π di machine learning

### Supervised o unsupervised

feature selection pu√≤ essere un'attivit√† sia unsupervised che supervised

**Supervised**:

- Filter methods
  - ad esempio: seleziono le feature con un'alta correlation with the target class
- Wrapper methods
- Embedded methods
  - Decision Trees (usano gli attributi che massimizzano l'information gain)
  - Lasso (L1)
  - Elastic Net

**NB**: in quanto supervised tutti questi metodi richiedono le label della target class

**Unsupervised:**

- Possiamo sfruttare clustering
  - Fai clustering (es. k-means) usando tutte le feature
  - Valuti la qualit√† del clustering
    - silhouette score
  - Rimuovi una feature
  - Rifai clustering
  - Se la qualit√† non peggiora (o migliora) ‚Üí la feature √® inutile
  - **NB**: questo √® un unsupervised wrapper method dato che usa kmeans come valutatore del subset di feature
  - **NB**: Nel supervised dici: ‚ÄúQuesta feature √® utile perch√© discrimina le classi‚Äù Nel clustering dici invece: ‚ÄúQuesta feature √® utile perch√© aiuta a mantenere separati i gruppi naturali di dati‚Äù
- Oppure possiamo sfruttare **PCA**
  - questa non √® propriamente feature selection, √® una combinazione di feature transformation e dimensionality reduction

### Filter and Wrapper methods

Nella Feature selection esistono diversi modi per scegliere un sottoinsieme di attributi utili

Filter e Wrapper ‚Äî rappresentano due grandi famiglie di tecniche per questo scopo

**Filter Approaches**:

I filter **selezionano le feature prima dell‚Äôapplicazione di qualsiasi algoritmo machine learning.**

- Esempio: Correlation-based selection -> si scelgono le feature pi√π correlate con il target.

**Wrapper approaches**:

I wrapper selezionano le feature utilizzando direttamente **un modello come ‚Äúvalutatore‚Äù.**

- Per ogni sottoinsieme di feature da testare, il modello viene addestrato e valutato
  - nota come questo sia molto **computazionalmente costoso** rispetto a filter methods
- il sottoinsieme che produce le performance migliori viene scelto

**Embedded Methods**:

Feature selection occurs naturally as part of the data mining algorithm

- e.g. decision trees oppure con Lasso regression

**Difference between Filter and Wrapper methods** (nel nostro caso):

```
dependant variable == variabile che vogliamo predire (che ci si aspetta dipenda in qualche modo dalle feature)
```

- Filter methods measure the relevance of features by their correlation with dependent variable
- wrapper methods measure the usefulness of a subset of feature by actually training a model on it

## Our correlation-based filter method

Correlation zero == absence of **linear** relationship between the variables

- limitation!
- the relationship might not be linear
  - nella slide 74, in the last line, there's obvious relationship between the features but the correlation is 0

We're going to use correlation this way:

- Identifying Redundant Features for Dimensionality reduction
  - Features highly correlated with each other contain overlapping information
  - Retain one feature from such groups to reduce dimensionality and overfitting

- Identifying Relevant Features (filtering)
  - High correlation with the target variable helps identify features with high predictive power
  - **NB**: low correlation between a feature and the target **can sometimes hide a non‚Äìlinear correlation**, therefore the mere use of low correlation for feature filtering can be dangerous
    - correlation should be complemented with other techniques to handle nonlinear relationships (non mi sembra per√≤ che abbiamo visto queste altre tecniche)

## Recursive Feature Elimination | Wrapper Method

RFE involves the following steps:

- Step 1: Train a model on all features of the dataset.
- Step 2: Rank features by importance
  - Based on the trained model, rank all features by their importance using model coefficients or feature importance scores
- Step 3: Remove the least important feature(s)
  - The model is retrained with the remaining features.
- Step 4: Repeat steps 1‚Äì3 until the desired number of features is reached

**NB**: Wrapper methods measure the usefulness of a subset of feature by actually training a model on it

- Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features
- tuttavia computationally expensive and no always easy to rank feature importance

# dimensionality-reduction

Instead of considering which subset of attributes is to be ignored it is possible to map the dataset into a new space with fewer attributes

**NB**: questa √® una trasformazione dei dati, non pi√π una semplice selezione

## PCA, Principal Component Analysis

```chatgpt
In PCA, **‚Äúcapturing the variability of the data‚Äù** significa:

‚û°Ô∏è **Rappresentare quanta parte dell‚Äôinformazione contenuta nei dati originali riusciamo a mantenere proiettandoli su una nuova direzione (una nuova dimensione).**

### Pi√π concretamente:

* La **variabilit√†** √® misurata tramite la **varianza**.
* Una direzione (o principal component) **cattura molta variabilit√†** se, quando proiettiamo i dati su quella direzione, i valori proiettati mostrano **alta varianza**.
* Alta varianza = punti pi√π ‚Äúsparpagliati‚Äù lungo quella direzione ‚Üí quella direzione contiene molta informazione sulla struttura dei dati.
* Proiettare i dati su una direzione significa considerarne solo la componente relativa a quella direzione


Le nuove dimensioni create dalla PCA:

1. Sono ordinate dalla maggiore alla minore varianza catturata.
2. Le prime poche componenti spesso spiegano la maggior parte dell‚Äôinformazione utile nei dati originali.
3. Questo permette di ridurre la dimensionalit√† senza perdere troppo contenuto informativo.

### Un esempio intuitivo

Immagina un insieme di punti disposti come un‚Äôellisse molto allungata:

* La direzione dell‚Äôasse lungo dell‚Äôellisse ha molta variabilit√† ‚Üí **prima componente principale**.
* La direzione corta ne ha meno ‚Üí **seconda componente**.

Se proietti i dati sulla prima direzione, preservi gi√† gran parte della forma dell‚Äôellisse ‚Üí hai **catturato molta variabilit√†**.

In breve: **"Catturare la variabilit√†" = mantenere quanto pi√π possibile la varianza dei dati originali quando li rappresenti in meno dimensioni.**

INOLTRE

In PCA non utilizzi le direzioni gi√† presenti nel dataset (cio√® gli assi originali degli attributi).
üëâ Devi calcolare delle nuove direzioni, chiamate componenti principali.

Gli assi originali non sono scelti per massimizzare la variabilit√†. Sono solo le coordinate in cui i dati sono stati raccolti.

PCA cerca invece le direzioni ‚Äúmigliori‚Äù per descrivere i dati, cio√®: quelle lungo cui i dati variano di pi√π (massima varianza) e che siano tra loro ortogonali (indipendenti)

Per calcolare queste nuove direzioni si fanno delle operazioni sulla matrice di covarianza e analisi sui suoi autovettori/valori
```

PCA √® computationally intensive, esistono forme approssimate pi√π fattibili per dataset grandi

## MultiDimensional Scaling

tecnica di visualizzazione

MDS is a technique used to visualize high-dimensional data in a low-dimensional space

Fits the projection of the elements into a m dimensional space in such a way that the distances among the elements are preserved

# Feature selection in Scikit-learn e cose pratiche che interessano a noi

## Univariate feature selection

√à una tecnica di feature selection supervisionata.

- Supervisionata perch√© usa il target (la variabile da predire).

‚ÄúUnivariate‚Äù significa che:

- ogni feature viene analizzata da sola
- si studia la relazione feature ‚Üî target
- non considera interazioni tra feature

Per ogni feature:

- applica un test statistico -> nel nostro caso correlazione
- ritorna:
  - uno score ‚Üí quanto la feature √® informativa nel predirre il target
  - un p-value ‚Üí quanto il risultato √® statisticamente significativo/affidabile

Infine, si selezionano le k feature con score migliore / una percenutale delle feature con score migliore

### p-value

It is the probability that the null hypothesis is acceptable

- the null hypothesis is that there is **no relationship between the feature and the target**

Il p-value serve a rispondere alla seguete domanda:

`Se in realt√† NON esistesse alcuna relazione tra X e y, quanto √® probabile osservare una correlazione almeno cos√¨ forte solo per caso?`

- es: lancio una moneta 10 volte e ottengo 9 teste e 1 croce. Possiamo concludere che p(testa) = 0.9?
- l'affidabilit√† dipende dal numero di volte che lancio la moneta.
- Il pvalue mi d√† una misura di affidabilit√† che in questo caso dipende dal numero di lanci

Pi√π il p-value √® piccolo pi√π la misura √® affidabile e non dovuta a casualit√†
